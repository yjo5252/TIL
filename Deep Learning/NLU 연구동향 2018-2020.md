## NLU 연구동향
Reference:[AI 트렌드2018-2020 NLU 연구 동향을 소개합니다](https://www.kakaobrain.com/blog/118)
#### 자연어처리(natural language processing, NLP) 분야의 핵심 과제
- 자연어이해(natural language understanding, NLU) : 자연어 형태의 문장을 이해하는 기술
- 자연어생성(natural language generation, NLG) : 자연어 문장을 생성하는 기술

다시 말해 NLU와 NLG는 사람과 기계, 그리고 기계 간 사람이 자연어로 소통하는 데 필요한 기술이라고 보면 됩니다.

 <b>사람과 기계의 상호 작용이 필수인 곳</b>에서 NLU은 핵심 기술이라고 할 수 있습니다.
- 구글(Google)과 같은 검색 서비스나 카카오미니와 같은 인공지능 스피커처럼
- 기계의 이해 능력이 좋아져야지만 사용자가 만족할 만한 수준의 정보나 답을 들려줄 수 있기 때문입니다.
- NLU 기술을 접목해 기존 키워드 매칭 방식과 비교해 더 나은 검색 서비스를 제공한 구글의 사례가 대표적입니다.


#### 언어 모델(language model)
1)비라벨링 데이터만 주어진 상태에서 입력 데이터 일부를 라벨로 사용하거나 사전 지식에 따라 라벨을 만들어 모델을 훈련하는 자기지도학습(self-supervised learning) 방법론이 발전하고, 
2)텍스트 훈련에 효과적인 딥러닝 모델 아키텍처가 개발됐으며,
3)GPU와 딥러닝 라이브러리가 발전한 덕분에 많은 양의 비라벨링 텍스트 데이터를 효과적으로 사전 훈련하는 모델 개발이 가능해졌습니다. 

#### 언어 모델의 성능 평가

언어 모델이 사람의 말을 얼마나 잘 이해하는지는 어떻게 평가해볼 수 있을까요?
앞에서 설명한 대로, 언어를 이해한다는 행위는 구체적으로 정의하기도, 검증하기도 어렵습니다. 
이에 전문가들은 정답이 명확하게 주어진 여러 문제를 고루 잘 푸는 모델을 기준으로 삼았습니다. 
다양한 과제를 잘 수행한다면 나중에 비슷한 종류의 새로운 데이터에서도 잘 작동한다고 가정했습니다.

- 언어 모델의 성능 평가에는 GLUE와 SuperGLUE라는 벤치마크가 주로 활용됩니다. 
  - 기존 9가지 라벨링 데이터셋을 모아 만든 GLUE는 다양한 과제에서 딥러닝 모델이 좋은 성능을 내는지를 살펴보는 데 방점을 둡니다.
  - 2018년 탄생한 사전학습된 딥러닝 기반 언어 모델(ELMo, GPT-1, BERT) 모두 GLUE 벤치마크에서 당시 최고의 성능을 달성했습니다.
  - 그중에서도 BERT를 업그레이드한 버전인 MT-DNN[5]와 RoBERTa는 GLUE 벤치마크를 기준으로 인간보다 뛰어난 성능을 내보이기도 했죠. 
- GLUE보다 ‘더 어려운’ 자연어이해 과제를 모은 벤치마크가 SuperGLUE입니다.
  -  8가지 데이터셋으로 구성된 SuperGLUE도 GLUE와 마찬가지로 다양한 NLU 문제를 해결할 수 있는지를 평가하는 데 주안을 두고 있습니다


#### 언어모델의 한계
- ‘BERT는 과제를 푼 게 아니다. 벤치마크를 달성한 것이다(solves benchmarks, not TASKs)’라는 표현
  - 실험 데이터에 존재하는 편향(bias)이나 아티팩트(artifact)와 같은 피상적인 단서를 익힌 모델에는 현실 문제를 제대로 풀지 못하는 맹점이 있을 수 있습니다.
  - 현실의 다양한 데이터를 활용한 추가학습을 하지 않은 이상 BERT 이후의 모델의 정확도는 25% 수준에 그치고 말죠. 
  - 하지만 그렇다고 해서 지금보다 더 나은 문제 정의 방식에 대해서도 그 누구도 이렇다 할 명쾌한 해답을 제시할 수 없다는 게 현실입니다.


- 물론 언어 모델은 사전 학습만으로도 외부 지식(external knowledge)을 일부 습득할 수는 있습니다. 
  - 언어 모델 그 자체를 지식 베이스의 일종이라고 볼 수 있다고 한 'Language Models as Knowledge Bases?'라는 논문이 바로 그 근거입니다. 
  - 키아누 리브스에 관한 최신 기사를 학습한 모델이라면 키아누 리브스에 관한 모든 정보를 알고 있을 수도 있다는 의미죠. 
  - 하지만 NLU에서 언어학적 지식 습득에 주안을 두는 언어 모델이 외부 지식 습득에는 한계가 있을 수밖에 없습니다.
  - 그러므로 좀 더 다양한 곳에서 활용할 수 있는 NLU 모델을 만들기 위해서는
    1)  <b> 외부 지식을 언어 모델에 내재화</b>하거나,
    2)  <b> 따로 저장한 외부 지식을 활용하는 학습 메커니즘</b>이 도입되어야 할 것으로 보입니다.


#### 2020년 NLU 트렌드는?

1. 앞서 언급된 일부 한계점을 극복하고자 이후 진행된 연구
-  관계 기반 질의응답(relation-based question-answering) 과제에서 별도의 지식 그래프(knowledge graph)를 이용해 사실 관계(entity relations)를 표현하려는는 시도가 있었습니다. 
- 이벤트 간의 관계(event relations)를 표현한 추론적 지식 그래프(inferential knowledge graph)를 언어 모델(GPT)과 조합해 추론 능력을 높이려는 시도 있었죠.

2. 언어이해 기능 경량화 연구
- 모델 학습 및 추론에 드는 시간과 자원을 획기적으로 줄여 언제 어디서나 언어이해 기능을 이용할 수 있도록 하는 경량화 연구인 MobileBERT나, SentenceBERT처럼 검색에 필요한 임베딩 방법론을 연구하는 트렌드를 비춰봤을 때 2020년 올 한해는 현실 속 다양한 NLU 과제를 푸는데 집중한 연구가 다수 진행될 것으로 예상됩니다. 
- 구글에서 자사 검색 서비스에 BERT를 적용한 사례에서처럼 말이죠.

3. 상용화
- 아울러 BERT를 상용화하는 데 수많은 과제를 해결하는 연구도 한층 더 활발할 것으로 보입니다. 
- BERT를 분석한 각종 논문을 살펴보면,
  - 어떤 데이터를 얼마나 사용해야 하는지, 
  - 어떻게 하면 모델을 효과적으로 훈련할 수 있을지, 
  - 어떻게 해야 편향과 아티팩트에도 흔들리지 않는 견고한 모델을 만들 수 있을지 등이 꾸준히 언급되고 있는 게 바로 그 증거죠. 
- 최신 벤치마크에서 최고의 성능을 달성한 거대한 모델이 연속해서 탄생한 가운데,
  -  각 모델이 무엇을 이해하지 못하는지,
  -  그리고 이를 보완할 방법에는 무엇이 있는지 등을 살펴보는 연구 역시 중요한 축에 해당할 것으로 보입니다. 
  -  더 잘 이해하는 모델인지를 평가하고자 SuperSuperGLUE와 같은 최신 벤치마크도 탄생하겠죠.





